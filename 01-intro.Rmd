---
output:
  word_document: default
  html_document: default
---
\newpage

# The Effectiveness of Psychotherapy Delivered in Routine Care Settings: A Systematic Review and Meta-Analysis {.unlisted .unnumbered}

# Introduction {.unnumbered}

Meta-analyses of clinical trials support the efficacy of psychotherapy for common mental health problems such as depression [@Cuijpers2008], anxiety disorders [e.g., @Cuijpers2014; @Mayo-Wilson2014; @Sanchez-Meca2010; @Wolitzky-Taylor2008; @Olatunji2014], post-traumatic stress disorder [@Lewis2020], obsessive-compulsive disorder [@Rosa-Alcazar2008], eating disorders [@Linardon2017], psychosis [@Turner2020] and other conditions. Grounded in this evidence, clinical guidelines endorse the use of psychological interventions in routine clinical care and support their inclusion in organized health care systems [e.g., @Chambless1998; @Chambless2001; @NationalCollaboratingCentreforMentalHealth2011]. These guidelines typically advocate the implementation of evidence-based models of psychotherapy, closely following the procedures implemented in clinical trials and specified in treatment manuals. To this end, competency frameworks have been developed to support the dissemination of empirically supported treatments through clinical training programmes and clinical supervision [e.g., @Lemma2008; @Roth2008; @Roth2009].

Despite its convincing experimental evidence base, there are reasons to expect that the effects of psychotherapy delivered in routine care settings may differ from those observed in clinical trials. Surveys of practicing clinicians reveal that many hold negative attitudes towards protocol-driven therapy and subsequently do not follow treatment manuals [e.g., @Addis2000]. Hence, the extent to which routinely delivered psychotherapy resembles trial-based empirically supported treatments is unclear [@Freedland2011]. It has also been argued that the strict selection criteria applied in clinical trials may result in unusually homogeneous samples that do not adequately reflect clinical populations that are typical of routine care settings [e.g., @Lambert2013a; @Zimmerman2002]. It has also been argued that using control conditions within practice-based psychotherapy poses ethical concerns (e.g., leaving severely distressed samples untreated) and therefore various patient subgroups are likely to be under represented [@Nordmo2020; @Philips2021]. Differences in the clinical profiles of patients included and excluded from psychotherapy trials have been demonstrated [e.g., @vanderLem2012a], some studies have nevertheless found similar clinical outcomes when comparing samples from efficacy trials and routine practice [@Lutz2016a; @Persons1999]. However, this adequate transportability and effectiveness of evidence-based interventions is unlikely to be a general rule across all routine care settings. For example, significant variability in clinical outcomes has been found between clinics that implemented protocol-driven and highly standardized interventions in England [@Gyani2013]. This variability may be partly explained by differences in clinical and demographic features of local populations (i.e., case-mix) and partly explained by implementation degrees of freedom: differences in how clinics manage treatment selection and treatment duration [@Clark2018]. For all of these reasons, it is plausible to assume that the effects of routinely delivered therapy may vary across settings and clinical populations, and may not necessarily conform to benchmarks from efficacy trials.

Given the above sources of uncertainty and variability, psychotherapy researchers have taken interest in evaluating the effectiveness of routinely delivered psychotherapy. A tradition of practice-based evidence [PBE, @Margison2000] has emerged in recent decades, with numerous studies that examine the effects of routine care psychological interventions in various settings. Narrative reviews of PBE generally confirm that moderate-to-large uncontrolled (pre-to-post treatment) effect sizes are observed in routine care settings, supporting the effectiveness of psychotherapy but also demonstrating considerable variability across patient samples, therapists and clinics [e.g., see @Barkham2010b; @Castonguay2013; @Castonguay2021]. An inherent limitation of such narrative reviews is that they perform a selective rather than systematic and comprehensive synthesis of available data. Benchmarking studies that pool practice-based data across multiple clinics tend to report favourable pooled effects sizes, but also variability in effects across clinics [e.g., @Barkham2001a; @Connell2007a; @Delgadillo2014a; @Gyani2013]. Although these studies help to quantify the expected magnitude of treatment effects observed in ordinary clinical settings, most are nevertheless circumscribed to small sets of clinics or geographical areas, offering limited insights into possible sources of heterogeneity in treatment outcomes. As such, systematic reviews and meta-analyses may be most illuminating.

Some meta-analytic investigations have reported that outcomes of clinic-based studies were not as favourable as those based in research settings [@Weisz1995]. Other meta-analyses suggest that there are no differences in treatment effects when comparing clinic-based and efficacy studies when case-mix differences are accounted for [e.g., @Shadish1997a; @Shadish2000]; however, many of the included clinic-based studies applied stringent controls on the treatment procedures â€“ making them more akin to lab-based studies. Later reviews have attempted to systematically identify available PBE studies and to derive generalizable information on the general effectiveness of psychotherapy in routine care. @Hunsley2007 reviewed 35 studies and concluded that the completion and improvement rates observed in PBE studies were comparable to those from efficacy trials. @Cahill2010 reviewed 31 studies, concluding that psychotherapy was most effective for the treatment of common mental disorders with a pooled uncontrolled effect size of *d* = 1.29. More recently, @Wakefield2021 reviewed 60 studies, of which 47 were eligible for meta-analysis. They reported large uncontrolled effect sizes for depression (*d* = 0.87) and anxiety (*d* = 0.88), and a moderate effect on functional impairment (*d* = 0.55). 

Heterogeneous effects are commonly found in psychotherapy meta-analyses, and can, to some extent, be explained through potential moderator variables [@Spielmans2018]. More recent PBE meta-analyses have started to provide insights into plausible methodological (e.g., completers analyses vs. inclusion of patients lost to follow-up) and clinical sources of heterogeneity in effectiveness (e.g., larger effects for common mental disorders, lower effects for patients with comorbidities and socioeconomic disadvantages, larger effects for lengthier interventions). Nevertheless, these meta-analyses are over a decade old [@Hunsley2007; @Cahill2010] or limited to a specific primary care setting [@Wakefield2021]. Further research into the methodological and clinical sources of treatment heterogeneity is needed for a more complete understanding of underlying differences in how people respond to treatment [@Spielmans2018].

Recent meta-analyses of efficacy trials have provided evidence for a range of potential moderators variables that are yet to be confirmed through meta-analysis of PBE. Methodological risk of bias criteria have often been associated with overestimates of treatment effect [for a review see @Munder2018]. @Barth2013a for example observed inflateded effect sizes for psychotherapy treatment studies of depression that: (i) lacked concealment of randomization, (ii) used non-blinded outcome assessors, and (iii) employed small samples. @Cuijpers2010 also found that exclusion of patients lost to follow-up (i.e., completer samples) produced inflated effect-sizes over studies that included such patients (i.e., intention-to-treat); although @Barth2013a found conflicting evidence.

Various clinical factors have been considered for their potential to explain heterogeneity of psychotherapy treatment findings [see @Spielmans2018 for a review]. For treatment modality (e.g., cognitive-behavioural, psychodynamic) recent meta-analyses of RCTs directly comparing different treatments have provided limited evidence that there are significant differences between treatments for depression [@Barth2013a; @Cuijpers2008]. Similar investigations in other conditions (e.g., generalised anxiety) have been restricted by the lack of evidence from direct comparison trials [@Cuijpers2014]. Aggregation of evidence for specific treatment modalities from non-comparative studies, although observational and highly susceptible to confounds, can allows for indirect comparisons of treatments.

Treatment dosage (or duration) is a clinically and economically relevant treatment variable however is often absent from psychotherapy meta-analytic reviews  [@Spielmans2018]. While there are various dosage prototypes available [see @Fluckiger2020] it is generally recommended that meta-analytic reviewers code it as a continuous variable [@Spielmans2018]. Several recent meta-analyses have failed to demonstrate a significant differential effect for dosage in depression [e.g., @Barth2013a; @Cuijpers2013], anxiety [e.g., @Carpenter2018] and OCD [e.g., @Olatunji2015]; although there remain exceptions to this rule [e.g., @Olatunji2014; @Turner2020]. Consideration of dosage within naturalistic settings is particularly relevant as the range of sessions is often treated more flexibly [@Lambert2013a] and varies between services and mental health systems [@Fluckiger2020].

Sample severity (i.e., clinical complexity of the sample) has been widely considered to be negatively associated with treatment effect. This has received meta-analytic support through comparatively smaller aggregate effect-sizes shown in meta-analyses of severe depression [e.g., @Cuijpers2010] and inpatient samples [@Cuijpers2011]. Differences between stratifications of depression severity have also been shown, with more severe samples showing smaller effect sizes [@Whiston2019].
Despite this, conflicting evidence has been reported through study level meta analyses of depression [@Driessen2010; @Cuijpers2014a], and health anxiety [@Olatunji2014] and independent patient level meta-analyses of depression [@Furukawa2017] and psychosis [@Turner2020]. @Furukawa2017 for example found that patients receiving CBT can benefit across the spectrum of pre-treatment severity of depression.
Varying levels of support have also been provided for differential rates of effectiveness based upon role of the outcome assessor [self-report vs clinician, @Cuijpers2014; @Cuijpers2008], sensitivity of measurement tool [e.g., @Cuijpers2014], and country. The degree to which these moderator variables findings are consistent within naturalistic settings remains unclear.

The considerable growth of the PBE literature in the last decade and implementation of empirically supported treatments across many settings warrants a comprehensive review and quantitative synthesis of the literature. This would enable us to gain a more precise understanding of [1] the magnitude of effect sizes across multiple outcome domains and [2] sources of heterogeneity. The aim of the present study was to systematically review available PBE studies using a meta-analytic synthesis of quantitative data and pre-specified moderator analyses informed by earlier studies. 

**Note:** The authors recognise that use of the term *effectiveness* may be somewhat misleading. The pre-post (uncontrolled) methodology which forms the body of evidence in this review is unable to disentangle treatments effects from other potential causes of change (e.g., regression to the mean, placebo). Observed change in symptoms may therefore not exclusively represent treatment effectiveness. We have opted to retain use of this term within the current review because it has frequently been employed in the extant literature  [e.g., @Lambert2013a; @Nordmo2020].
